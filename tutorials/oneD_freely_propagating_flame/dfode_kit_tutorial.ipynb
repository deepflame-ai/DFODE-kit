{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44caac4b",
   "metadata": {},
   "source": [
    "This is a tutorial for the DFODE-kit package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84b1a510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data_save/xyc/DFODE/DFODE-kit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dfode_kit.df_interface import (\n",
    "    OneDFreelyPropagatingFlameConfig,\n",
    "    setup_one_d_flame_case,\n",
    "    df_to_h5,\n",
    ")\n",
    "from dfode_kit.data_operations import (\n",
    "    touch_h5, \n",
    "    get_TPY_from_h5, \n",
    "    random_perturb,\n",
    "    label_npy,\n",
    "    integrate_h5,\n",
    "    calculate_error,\n",
    ")\n",
    "from dfode_kit.dfode_core.model.mlp import MLP\n",
    "from dfode_kit.utils import BCT\n",
    "\n",
    "DFODE_ROOT = os.environ['DFODE_ROOT']\n",
    "\n",
    "print(DFODE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032661c",
   "metadata": {},
   "source": [
    "### A brief introduction to the DFODE method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a592b7b",
   "metadata": {},
   "source": [
    "#### Low-dimensional manifold sampling\n",
    "\n",
    "A key challenge in preparing training data is achieving sufficient coverage of the relevant thermochemical composition space, which is often prohibitively high-dimensional when detailed chemistry involves tens to hundreds of species. \n",
    "\n",
    "To address this, DFODE-kit adopts a low-dimensional\n",
    "manifold sampling strategy, where thermochemical states are extracted from canonical flame configurations that retain the essential topology of high-dimensional turbulent flames. This approach ensures both computational efficiency and physical representativeness of the training datasets.\n",
    "\n",
    "In this tutorial, we will demonstrate how to use DFODE-kit to sample a low-dimensional manifold of thermochemical states from a one-dimensional laminar freely propagating flame simulated with DeepFlame. The following code block could also be found in `case_init.ipynb` files within the case templates provides in the `cases` directory. It is used to initialize the simulation and update the dictionary files for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58d8394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving premixed flame...\n",
      "Laminar Flame Speed      :   2.3489328863 m/s\n",
      "Laminar Flame Thickness  :   0.0003694362 m\n",
      "One-dimensional flame case setup completed at: /data_save/xyc/DFODE/DFODE-kit/tutorials/oneD_freely_propagating_flame\n"
     ]
    }
   ],
   "source": [
    "# Operating condition settings\n",
    "config_dict = {\n",
    "    \"mechanism\": f\"{DFODE_ROOT}/mechanisms/Burke2012_s9r23.yaml\",\n",
    "    \"T0\": 300,\n",
    "    \"p0\": 101325,\n",
    "    \"fuel\": \"H2:1\",\n",
    "    \"oxidizer\": \"O2:0.21,N2:0.79\",\n",
    "    \"eq_ratio\": 1.0,\n",
    "}\n",
    "config = OneDFreelyPropagatingFlameConfig(**config_dict)\n",
    "\n",
    "# Simulation settings\n",
    "settings = {\n",
    "    \"sim_time_step\": 1e-6,\n",
    "    \"sim_write_interval\": 1e-5,\n",
    "    \"num_output_steps\": 10,\n",
    "}\n",
    "config.update_config(settings)\n",
    "\n",
    "# Setup the case and update dictionary files\n",
    "setup_one_d_flame_case(config, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbe369",
   "metadata": {},
   "source": [
    "Note that at the point, the simulation is not yet started. The user would need to ensure a working version of DeepFlame is available and run the `Allrun` script from command line to start the simulation.\n",
    "\n",
    "```bash\n",
    "./Allrun\n",
    "```\n",
    "\n",
    "After the simulation is completed, we proceed to use DFODE-kit to gather and manage the thermochemical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd60205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species names: ['T', 'p', 'H', 'H2', 'O', 'OH', 'H2O', 'O2', 'HO2', 'H2O2', 'N2']\n",
      "Saved concatenated arrays to /data_save/xyc/DFODE/DFODE-kit/tutorials/oneD_freely_propagating_flame/tutorial_data.h5\n"
     ]
    }
   ],
   "source": [
    "df_to_h5(\n",
    "    root_dir=f\"{DFODE_ROOT}/tutorials/oneD_freely_propagating_flame\",\n",
    "    mechanism=f\"{DFODE_ROOT}/mechanisms/Burke2012_s9r23.yaml\",\n",
    "    hdf5_file_path=f\"{DFODE_ROOT}/tutorials/oneD_freely_propagating_flame/tutorial_data.h5\",\n",
    "    include_mesh=True,\n",
    ")\n",
    "\n",
    "# The above is equivalent to the following cli command:\n",
    "# dfode-kit sample --mech ../../mechanisms/Burke2012_s9r23.yaml \\\n",
    "#     --case . \\\n",
    "#     --save ./tutorial_data.h5 --include_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991ab21",
   "metadata": {},
   "source": [
    "Checking the contents of the h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b45c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting HDF5 file: tutorial_data.h5\n",
      "\n",
      "Metadata in the HDF5 file:\n",
      "mechanism: /data_save/xyc/DFODE/DFODE-kit/mechanisms/Burke2012_s9r23.yaml\n",
      "root_directory: /data_save/xyc/DFODE/DFODE-kit/tutorials/oneD_freely_propagating_flame\n",
      "species_names: ['T' 'p' 'H' 'H2' 'O' 'OH' 'H2O' 'O2' 'HO2' 'H2O2' 'N2']\n",
      "\n",
      "Groups and datasets in the HDF5 file:\n",
      "Group: mesh\n",
      "  Dataset: Cx, Shape: (500, 1)\n",
      "  Dataset: Cy, Shape: (500, 1)\n",
      "  Dataset: Cz, Shape: (500, 1)\n",
      "  Dataset: V, Shape: (500, 1)\n",
      "Group: scalar_fields\n",
      "  Dataset: 0.0001, Shape: (500, 11)\n",
      "  Dataset: 0.00011, Shape: (500, 11)\n",
      "  Dataset: 1e-05, Shape: (500, 11)\n",
      "  Dataset: 2e-05, Shape: (500, 11)\n",
      "  Dataset: 3e-05, Shape: (500, 11)\n",
      "  Dataset: 4e-05, Shape: (500, 11)\n",
      "  Dataset: 5e-05, Shape: (500, 11)\n",
      "  Dataset: 6e-05, Shape: (500, 11)\n",
      "  Dataset: 7e-05, Shape: (500, 11)\n",
      "  Dataset: 8e-05, Shape: (500, 11)\n",
      "  Dataset: 9e-05, Shape: (500, 11)\n"
     ]
    }
   ],
   "source": [
    "touch_h5(\"tutorial_data.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779899a3",
   "metadata": {},
   "source": [
    "#### Data augmentation and labeling\n",
    "\n",
    "While laminar canonical flames provide fundamental thermochemical states,their trajectory-aligned sampling in composition space poses significant limitations for a posteriori modeling applications. First, these sampled states are confined to predefined flamelet manifolds, making the trained model highly sensitive to perturbations and leading to an over-constrained representation. Second, the sampled states span a lower-dimensional subspace, which fails to encompass the full range of thermochemical variations encountered in turbulent combustion. As a result, the model becomes vulnerable to off-manifold perturbationsâ€”deviations from the training manifold that frequently arise in turbulent reacting flows.\n",
    "\n",
    "To tackle this challenge, a data augmentation strategy is employed, where collected states are perturbed to simulate the effects of multi-dimensional transport and turbulence disturbances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ece01cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from h5 file: tutorial_data.h5\n",
      "Number of datasets in scalar_fields group: 11\n",
      "Data shape: (5500, 11)\n",
      "5414\n",
      "10822\n",
      "16220\n",
      "21621\n",
      "(20000, 11)\n",
      "Saved augmented data shape: (20000, 11)\n",
      "Saved augmented data to data\n"
     ]
    }
   ],
   "source": [
    "# thermochemical_data = get_TPY_from_h5(\"tutorial_data.h5\")\n",
    "# print(thermochemical_data[0])\n",
    "\n",
    "# aug_thermochemical_data = random_perturb(thermochemical_data)\n",
    "# print(aug_thermochemical_data[0])\n",
    "h5_file = 'tutorial_data.h5'\n",
    "mech = f'{DFODE_ROOT}/mechanisms/Burke2012_s9r23.yaml'\n",
    "dataset_num = 20000\n",
    "output_file = 'data'\n",
    "\n",
    "print(f\"Loading data from h5 file: {h5_file}\")\n",
    "data = get_TPY_from_h5(h5_file)    \n",
    "print(\"Data shape:\", data.shape)\n",
    "All_data = random_perturb(data, mech, dataset_num, heat_limit=False, element_limit=True)\n",
    "np.save(output_file, All_data)\n",
    "print(\"Saved augmented data shape:\", All_data.shape)\n",
    "print(f\"Saved augmented data to {output_file}\")\n",
    "\n",
    "# The above is equivalent to the following cli command:\n",
    "# dfode-kit augment --mech ../../mechanisms/Burke2012_s9r23.yaml \\\n",
    "#     --h5_file ./tutorial_data.h5 \\\n",
    "#     --output_file ./data \\\n",
    "#     --dataset_num 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f2d8f",
   "metadata": {},
   "source": [
    "The CVODE integrator from Cantera is used for time integration and to provide supervised learning labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff47da26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from: ./data.npy\n",
      "test_data.shape=(20000, 11)\n",
      "Total time used: 4.83 seconds\n",
      "Labeled data saved to: dataset.npy\n"
     ]
    }
   ],
   "source": [
    "from dfode_kit.data_operations import label_npy\n",
    "\n",
    "try:\n",
    "    labeled_data = label_npy(\n",
    "        mech_path=mech,\n",
    "        time_step=1e-06,\n",
    "        source_path='./data.npy'\n",
    "    )\n",
    "    np.save('dataset.npy', labeled_data)\n",
    "    print(f\"Labeled data saved to: {'dataset.npy'}\")\n",
    "    \n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# The above is equivalent to the following cli command:\n",
    "# dfode-kit label --mech ../../mechanisms/Burke2012_s9r23.yaml \\\n",
    "#     --time 1e-06 \\\n",
    "#     --source ./data.npy \\\n",
    "#     --save ./dataset.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc291e95",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "Only a demo for training a model is provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d5aa0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.16250306e+08  1.21420458e+03  1.55758911e+07  2.19024943e+06\n",
      " -1.34247196e+07 -2.64955842e+01  3.80292325e+05 -4.00154553e+06\n",
      "  5.10073129e+01]\n",
      "(20000, 11) (20000, 11)\n",
      "Epoch: 1, Loss1: 8.564788e-01, Loss2: 9.387764e-05, Loss3: 5.497768e+09, Loss: 8.571225e-01\n",
      "Epoch: 2, Loss1: 8.271113e-01, Loss2: 9.650315e-05, Loss3: 5.387294e+09, Loss: 8.277465e-01\n",
      "Epoch: 3, Loss1: 7.887651e-01, Loss2: 1.006542e-04, Loss3: 5.255972e+09, Loss: 7.893914e-01\n",
      "Epoch: 4, Loss1: 7.281737e-01, Loss2: 1.060234e-04, Loss3: 5.043246e+09, Loss: 7.287840e-01\n",
      "Epoch: 5, Loss1: 6.564120e-01, Loss2: 1.112327e-04, Loss3: 4.684377e+09, Loss: 6.569917e-01\n",
      "Epoch: 6, Loss1: 5.635148e-01, Loss2: 1.122834e-04, Loss3: 4.235208e+09, Loss: 5.640507e-01\n",
      "Epoch: 7, Loss1: 4.457512e-01, Loss2: 9.024931e-05, Loss3: 3.226162e+09, Loss: 4.461641e-01\n",
      "Epoch: 8, Loss1: 4.088077e-01, Loss2: 1.369758e-04, Loss3: 1.343138e+09, Loss: 4.090790e-01\n",
      "Epoch: 9, Loss1: 3.196921e-01, Loss2: 1.628706e-04, Loss3: 6.879473e+08, Loss: 3.199238e-01\n",
      "Epoch: 10, Loss1: 2.490344e-01, Loss2: 1.686112e-04, Loss3: 1.262838e+09, Loss: 2.493293e-01\n",
      "Epoch: 11, Loss1: 2.567045e-01, Loss2: 1.156194e-04, Loss3: 1.373850e+09, Loss: 2.569576e-01\n",
      "Epoch: 12, Loss1: 2.365044e-01, Loss2: 6.607724e-05, Loss3: 1.248722e+09, Loss: 2.366953e-01\n",
      "Epoch: 13, Loss1: 2.106346e-01, Loss2: 5.031324e-05, Loss3: 9.414576e+08, Loss: 2.107791e-01\n",
      "Epoch: 14, Loss1: 1.547667e-01, Loss2: 8.556023e-05, Loss3: 9.957178e+08, Loss: 1.549519e-01\n",
      "Epoch: 15, Loss1: 1.618520e-01, Loss2: 1.272450e-04, Loss3: 8.376514e+08, Loss: 1.620630e-01\n",
      "Epoch: 16, Loss1: 1.608001e-01, Loss2: 1.568593e-04, Loss3: 1.062591e+09, Loss: 1.610632e-01\n",
      "Epoch: 17, Loss1: 1.506261e-01, Loss2: 1.704730e-04, Loss3: 1.380648e+09, Loss: 1.509347e-01\n",
      "Epoch: 18, Loss1: 1.517070e-01, Loss2: 1.586094e-04, Loss3: 1.512828e+09, Loss: 1.520169e-01\n",
      "Epoch: 19, Loss1: 1.501746e-01, Loss2: 1.280274e-04, Loss3: 1.368354e+09, Loss: 1.504395e-01\n",
      "Epoch: 20, Loss1: 1.334531e-01, Loss2: 1.003067e-04, Loss3: 1.055822e+09, Loss: 1.336589e-01\n",
      "Epoch: 21, Loss1: 1.014903e-01, Loss2: 7.924322e-05, Loss3: 8.579492e+08, Loss: 1.016554e-01\n",
      "Epoch: 22, Loss1: 7.775688e-02, Loss2: 5.050428e-05, Loss3: 8.380494e+08, Loss: 7.789119e-02\n",
      "Epoch: 23, Loss1: 9.128799e-02, Loss2: 8.478453e-06, Loss3: 1.110372e+09, Loss: 9.140750e-02\n",
      "Epoch: 24, Loss1: 1.077927e-01, Loss2: 3.597284e-05, Loss3: 1.143612e+09, Loss: 1.079430e-01\n",
      "Epoch: 25, Loss1: 1.039602e-01, Loss2: 6.042400e-05, Loss3: 9.236388e+08, Loss: 1.041130e-01\n",
      "Epoch: 26, Loss1: 8.681269e-02, Loss2: 7.192638e-05, Loss3: 9.204244e+08, Loss: 8.697666e-02\n",
      "Epoch: 27, Loss1: 7.050434e-02, Loss2: 8.172040e-05, Loss3: 7.672195e+08, Loss: 7.066278e-02\n",
      "Epoch: 28, Loss1: 7.195235e-02, Loss2: 8.173480e-05, Loss3: 1.096144e+09, Loss: 7.214370e-02\n",
      "Epoch: 29, Loss1: 7.398903e-02, Loss2: 7.937124e-05, Loss3: 1.070340e+09, Loss: 7.417544e-02\n",
      "Epoch: 30, Loss1: 7.925814e-02, Loss2: 7.894378e-05, Loss3: 8.510006e+08, Loss: 7.942218e-02\n",
      "Epoch: 31, Loss1: 6.062980e-02, Loss2: 8.610352e-05, Loss3: 4.275895e+08, Loss: 6.075867e-02\n",
      "Epoch: 32, Loss1: 5.735746e-02, Loss2: 7.744163e-05, Loss3: 2.675736e+08, Loss: 5.746166e-02\n",
      "Epoch: 33, Loss1: 6.438133e-02, Loss2: 6.706819e-05, Loss3: 6.170564e+08, Loss: 6.451011e-02\n",
      "Epoch: 34, Loss1: 7.142378e-02, Loss2: 4.990336e-05, Loss3: 6.444579e+08, Loss: 7.153813e-02\n",
      "Epoch: 35, Loss1: 6.375456e-02, Loss2: 3.640977e-05, Loss3: 4.123464e+08, Loss: 6.383220e-02\n",
      "Epoch: 36, Loss1: 5.306340e-02, Loss2: 3.932520e-05, Loss3: 1.935064e+08, Loss: 5.312207e-02\n",
      "Epoch: 37, Loss1: 4.818014e-02, Loss2: 3.345861e-05, Loss3: 3.511833e+08, Loss: 4.824871e-02\n",
      "Epoch: 38, Loss1: 5.914789e-02, Loss2: 3.687894e-05, Loss3: 3.966720e+08, Loss: 5.922443e-02\n",
      "Epoch: 39, Loss1: 4.676690e-02, Loss2: 3.942751e-05, Loss3: 2.808295e+08, Loss: 4.683441e-02\n",
      "Epoch: 40, Loss1: 4.956524e-02, Loss2: 2.996055e-05, Loss3: 3.283588e+08, Loss: 4.962804e-02\n",
      "Epoch: 41, Loss1: 5.351573e-02, Loss2: 2.574063e-05, Loss3: 3.266221e+08, Loss: 5.357414e-02\n",
      "Epoch: 42, Loss1: 5.886461e-02, Loss2: 2.217271e-05, Loss3: 3.134234e+08, Loss: 5.891813e-02\n",
      "Epoch: 43, Loss1: 5.833945e-02, Loss2: 3.138473e-05, Loss3: 2.337074e+08, Loss: 5.839421e-02\n",
      "Epoch: 44, Loss1: 5.631578e-02, Loss2: 4.255239e-05, Loss3: 4.326190e+08, Loss: 5.640159e-02\n",
      "Epoch: 45, Loss1: 6.143712e-02, Loss2: 4.959727e-05, Loss3: 3.830865e+08, Loss: 6.152502e-02\n",
      "Epoch: 46, Loss1: 5.307749e-02, Loss2: 5.102922e-05, Loss3: 3.275598e+08, Loss: 5.316127e-02\n",
      "Epoch: 47, Loss1: 5.061817e-02, Loss2: 4.024501e-05, Loss3: 2.883297e+08, Loss: 5.068725e-02\n",
      "Epoch: 48, Loss1: 5.382935e-02, Loss2: 1.341673e-05, Loss3: 4.871817e+08, Loss: 5.389149e-02\n",
      "Epoch: 49, Loss1: 5.325776e-02, Loss2: 2.853532e-05, Loss3: 5.041330e+08, Loss: 5.333671e-02\n",
      "Epoch: 50, Loss1: 5.296388e-02, Loss2: 4.909572e-05, Loss3: 4.756558e+08, Loss: 5.306054e-02\n",
      "Epoch: 51, Loss1: 4.261588e-02, Loss2: 5.100126e-05, Loss3: 3.329031e+08, Loss: 4.270017e-02\n",
      "Epoch: 52, Loss1: 5.465194e-02, Loss2: 4.019758e-05, Loss3: 4.341152e+08, Loss: 5.473555e-02\n",
      "Epoch: 53, Loss1: 5.137762e-02, Loss2: 2.465508e-05, Loss3: 4.788923e+08, Loss: 5.145017e-02\n",
      "Epoch: 54, Loss1: 3.550430e-02, Loss2: 2.396190e-05, Loss3: 3.034580e+08, Loss: 3.555861e-02\n",
      "Epoch: 55, Loss1: 4.699625e-02, Loss2: 2.759345e-05, Loss3: 2.544600e+08, Loss: 4.704929e-02\n",
      "Epoch: 56, Loss1: 5.518998e-02, Loss2: 2.193650e-05, Loss3: 3.618810e+08, Loss: 5.524811e-02\n",
      "Epoch: 57, Loss1: 4.636641e-02, Loss2: 2.184683e-05, Loss3: 3.230156e+08, Loss: 4.642056e-02\n",
      "Epoch: 58, Loss1: 4.187954e-02, Loss2: 2.491207e-05, Loss3: 3.384364e+08, Loss: 4.193829e-02\n",
      "Epoch: 59, Loss1: 5.144419e-02, Loss2: 2.222477e-05, Loss3: 4.125710e+08, Loss: 5.150767e-02\n",
      "Epoch: 60, Loss1: 5.246584e-02, Loss2: 1.489969e-05, Loss3: 4.654975e+08, Loss: 5.252730e-02\n",
      "Epoch: 61, Loss1: 3.501247e-02, Loss2: 5.404655e-06, Loss3: 2.924628e+08, Loss: 3.504713e-02\n",
      "Epoch: 62, Loss1: 3.271077e-02, Loss2: 1.614357e-05, Loss3: 2.138911e+08, Loss: 3.274830e-02\n",
      "Epoch: 63, Loss1: 4.265328e-02, Loss2: 2.426307e-05, Loss3: 4.513173e+08, Loss: 4.272267e-02\n",
      "Epoch: 64, Loss1: 4.972029e-02, Loss2: 2.271315e-05, Loss3: 5.078521e+08, Loss: 4.979379e-02\n",
      "Epoch: 65, Loss1: 4.018821e-02, Loss2: 1.924821e-05, Loss3: 4.859652e+08, Loss: 4.025605e-02\n",
      "Epoch: 66, Loss1: 3.212764e-02, Loss2: 2.040144e-05, Loss3: 3.204828e+08, Loss: 3.218009e-02\n",
      "Epoch: 67, Loss1: 3.827586e-02, Loss2: 2.100544e-05, Loss3: 3.229156e+08, Loss: 3.832916e-02\n",
      "Epoch: 68, Loss1: 3.874201e-02, Loss2: 2.124328e-05, Loss3: 4.878036e+08, Loss: 3.881203e-02\n",
      "Epoch: 69, Loss1: 3.785621e-02, Loss2: 2.003158e-05, Loss3: 5.000316e+08, Loss: 3.792624e-02\n",
      "Epoch: 70, Loss1: 3.126244e-02, Loss2: 2.043255e-05, Loss3: 2.719029e+08, Loss: 3.131006e-02\n",
      "Epoch: 71, Loss1: 3.907650e-02, Loss2: 1.042912e-05, Loss3: 1.113163e+08, Loss: 3.909806e-02\n",
      "Epoch: 72, Loss1: 2.946686e-02, Loss2: 1.664184e-05, Loss3: 6.937634e+07, Loss: 2.949044e-02\n",
      "Epoch: 73, Loss1: 3.625569e-02, Loss2: 2.485155e-05, Loss3: 2.665956e+08, Loss: 3.630720e-02\n",
      "Epoch: 74, Loss1: 3.933758e-02, Loss2: 2.848253e-05, Loss3: 2.583755e+08, Loss: 3.939190e-02\n",
      "Epoch: 75, Loss1: 4.067729e-02, Loss2: 3.222330e-05, Loss3: 2.791678e+08, Loss: 4.073744e-02\n",
      "Epoch: 76, Loss1: 3.807703e-02, Loss2: 2.959560e-05, Loss3: 2.654826e+08, Loss: 3.813317e-02\n",
      "Epoch: 77, Loss1: 2.315415e-02, Loss2: 1.556403e-05, Loss3: 1.079311e+08, Loss: 2.318051e-02\n",
      "Epoch: 78, Loss1: 3.286791e-02, Loss2: 1.428880e-05, Loss3: 2.677770e+08, Loss: 3.290898e-02\n",
      "Epoch: 79, Loss1: 3.161635e-02, Loss2: 2.597494e-05, Loss3: 3.870032e+08, Loss: 3.168102e-02\n",
      "Epoch: 80, Loss1: 3.742945e-02, Loss2: 4.010234e-05, Loss3: 3.511898e+08, Loss: 3.750467e-02\n",
      "Epoch: 81, Loss1: 4.571576e-02, Loss2: 4.802357e-05, Loss3: 2.496986e+08, Loss: 4.578876e-02\n",
      "Epoch: 82, Loss1: 3.901388e-02, Loss2: 5.316763e-05, Loss3: 2.534153e+08, Loss: 3.909239e-02\n",
      "Epoch: 83, Loss1: 3.989077e-02, Loss2: 3.727825e-05, Loss3: 3.544805e+08, Loss: 3.996350e-02\n",
      "Epoch: 84, Loss1: 3.346362e-02, Loss2: 2.245141e-05, Loss3: 2.581087e+08, Loss: 3.351188e-02\n",
      "Epoch: 85, Loss1: 2.924832e-02, Loss2: 1.874344e-05, Loss3: 1.363325e+08, Loss: 2.928069e-02\n",
      "Epoch: 86, Loss1: 4.225669e-02, Loss2: 2.759702e-05, Loss3: 1.686121e+08, Loss: 4.230115e-02\n",
      "Epoch: 87, Loss1: 3.549338e-02, Loss2: 3.589282e-05, Loss3: 1.410810e+08, Loss: 3.554338e-02\n",
      "Epoch: 88, Loss1: 3.335166e-02, Loss2: 4.627056e-05, Loss3: 2.044096e+08, Loss: 3.341837e-02\n",
      "Epoch: 89, Loss1: 3.293096e-02, Loss2: 4.843321e-05, Loss3: 3.012362e+08, Loss: 3.300951e-02\n",
      "Epoch: 90, Loss1: 2.775862e-02, Loss2: 3.902204e-05, Loss3: 3.546259e+08, Loss: 2.783311e-02\n",
      "Epoch: 91, Loss1: 3.022895e-02, Loss2: 2.467559e-05, Loss3: 4.047520e+08, Loss: 3.029410e-02\n",
      "Epoch: 92, Loss1: 3.211873e-02, Loss2: 1.577016e-05, Loss3: 3.090161e+08, Loss: 3.216540e-02\n",
      "Epoch: 93, Loss1: 3.302878e-02, Loss2: 2.016911e-05, Loss3: 3.047946e+08, Loss: 3.307943e-02\n",
      "Epoch: 94, Loss1: 2.976410e-02, Loss2: 2.919640e-05, Loss3: 3.701249e+08, Loss: 2.983031e-02\n",
      "Epoch: 95, Loss1: 3.369369e-02, Loss2: 3.485934e-05, Loss3: 4.764503e+08, Loss: 3.377619e-02\n",
      "Epoch: 96, Loss1: 3.092976e-02, Loss2: 3.275547e-05, Loss3: 3.642932e+08, Loss: 3.099894e-02\n",
      "Epoch: 97, Loss1: 2.468910e-02, Loss2: 1.672514e-05, Loss3: 2.513033e+08, Loss: 2.473095e-02\n",
      "Epoch: 98, Loss1: 3.314383e-02, Loss2: 6.906274e-06, Loss3: 3.217183e+08, Loss: 3.318291e-02\n",
      "Epoch: 99, Loss1: 2.400059e-02, Loss2: 1.481761e-05, Loss3: 3.767257e+08, Loss: 2.405307e-02\n",
      "Epoch: 100, Loss1: 3.500246e-02, Loss2: 1.866786e-05, Loss3: 5.260821e+08, Loss: 3.507373e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cantera as ct\n",
    "from dfode_kit.dfode_core.train.formation import formation_calculate\n",
    "\n",
    "source_file = 'dataset.npy'\n",
    "time_step = 1e-06\n",
    "output_path = './demo_model.pt'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "labeled_data = np.load(source_file)\n",
    "\n",
    "gas = ct.Solution(mech)\n",
    "n_species = gas.n_species\n",
    "formation_enthalpies = formation_calculate(mech)\n",
    "\n",
    "# Model instantiation\n",
    "demo_model = MLP([2+n_species, 400, 400, 400, 400, n_species-1]).to(device)\n",
    "\n",
    "# Data loading\n",
    "thermochem_states1 = labeled_data[:, 0:2+n_species]\n",
    "thermochem_states2 = labeled_data[:, 2+n_species:]\n",
    "\n",
    "print(thermochem_states1.shape, thermochem_states2.shape)\n",
    "thermochem_states1[:, 2:] = np.clip(thermochem_states1[:, 2:], 0, 1)\n",
    "thermochem_states2[:, 2:] = np.clip(thermochem_states2[:, 2:], 0, 1)\n",
    "\n",
    "features = torch.tensor(BCT(thermochem_states1), dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(BCT(thermochem_states2[:, 2:-1]) - BCT(thermochem_states1[:, 2:-1]), dtype=torch.float32).to(device)\n",
    "\n",
    "features_mean = torch.mean(features, dim=0)\n",
    "features_std = torch.std(features, dim=0)\n",
    "features = (features - features_mean) / features_std\n",
    "\n",
    "labels_mean = torch.mean(labels, dim=0)\n",
    "labels_std = torch.std(labels, dim=0)\n",
    "labels = (labels - labels_mean) / labels_std\n",
    "\n",
    "formation_enthalpies = torch.tensor(formation_enthalpies, dtype=torch.float32).to(device)\n",
    "\n",
    "# Training\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(demo_model.parameters(), lr=1e-3)\n",
    "\n",
    "demo_model.train()  \n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    preds = demo_model(features)\n",
    "    loss1 = loss_fn(preds, labels)   ## LOSS  \n",
    "\n",
    "    Y_in = ((features[:,2:-1]*features_std[2:-1] + features_mean[2:-1])*0.1 + 1)**10\n",
    "    Y_out = (((preds*labels_std + labels_mean) + (features[:,2:-1]*features_std[2:-1] + features_mean[2:-1]))*0.1 + 1)**10\n",
    "    Y_target = (((labels*labels_std + labels_mean) + (features[:,2:-1]*features_std[2:-1] + features_mean[2:-1]))*0.1 + 1)**10\n",
    "    loss2 = loss_fn(Y_out.sum(axis=1), Y_in.sum(axis=1))\n",
    "\n",
    "    Y_out_total = torch.cat((Y_out, (1-Y_out.sum(axis=1)).reshape(Y_out.shape[0],1)), axis = 1)\n",
    "    Y_target_total = torch.cat((Y_target, (1-Y_target.sum(axis=1)).reshape(Y_target.shape[0],1)), axis = 1)\n",
    "    loss3 = loss_fn((formation_enthalpies*Y_out_total).sum(axis=1), (formation_enthalpies*Y_target_total).sum(axis=1))/time_step\n",
    "\n",
    "    loss = loss1 + loss2 + loss3/1e+13\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: {}, Loss1: {:4e}, Loss2: {:4e}, Loss3: {:4e}, Loss: {:4e}\".format(epoch+1, loss1.item(), loss2.item(), loss3.item(), loss.item()))\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        'net': demo_model.state_dict(),\n",
    "        'data_in_mean': features_mean.cpu().numpy(),\n",
    "        'data_in_std': features_std.cpu().numpy(),\n",
    "        'data_target_mean': labels_mean.cpu().numpy(),\n",
    "        'data_target_std': labels_std.cpu().numpy(),\n",
    "    },\n",
    "    output_path\n",
    ")\n",
    "\n",
    "# The above is equivalent to the following cli command:\n",
    "# dfode-kit train --mech ../../mechanisms/Burke2012_s9r23.yaml     \\\n",
    "#     --source_file ./dataset.npy     \\\n",
    "#     --output_path ./demo_model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
